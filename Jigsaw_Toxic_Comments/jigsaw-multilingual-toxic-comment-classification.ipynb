{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/jigsaw-translated/test_en.csv\n/kaggle/input/jigsaw-translated/validation_en.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install transformers","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (2.7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.18.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.38)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.12.32)\nRequirement already satisfied: tokenizers==0.5.2 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.5.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2020.2.20)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.85)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.7)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.42.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\nRequirement already satisfied: botocore<1.16.0,>=1.15.32 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (1.15.32)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers) (0.9.5)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->transformers) (0.15.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.32->boto3->transformers) (2.8.1)\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"device_type = 'gpu' # or gpu, or cpu or tpu\ntranslated = True","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport numpy as np\nimport io\nimport os\nimport time\nimport random\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, confusion_matrix, roc_auc_score\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom transformers import *","execution_count":4,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"if device_type == 'tpu':\n    !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n    import torch_xla\n    import torch_xla.core.xla_model as xm","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nif device_type == 'gpu':\n    ## Set seed of randomization and working device\n    manual_seed = 77\n    torch.manual_seed(manual_seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n    n_gpu = torch.cuda.device_count()\n    if n_gpu > 0:\n        torch.cuda.manual_seed(manual_seed)\n        print(torch.cuda.get_device_name(0))\nelif device_type == 'tpu':\n    seed = 777\n    seed_everything(seed)\n    device = xm.xla_device()\n    print(device)","execution_count":6,"outputs":[{"output_type":"stream","text":"cuda\nTesla P100-PCIE-16GB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELS = [(BertModel,       BertTokenizer,       'bert-base-multilingual-cased'),\n          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-large'),\n         ]","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DATA PREPROCESSING"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_folder = \"/kaggle/input/jigsaw-multilingual-toxic-comment-classification\"\ntrain_toxic_comment_path = os.path.join(data_folder, 'jigsaw-toxic-comment-train.csv')\n# train_unintended_bias_path = os.path.join(data_folder, 'jigsaw-unintended-bias-train.csv')\nif translated:\n    valid_path = os.path.join('/kaggle/input/jigsaw-translated', 'validation_en.csv')\n    test_path = os.path.join('/kaggle/input/jigsaw-translated', 'test_en.csv')\nelse:\n    valid_path = os.path.join(data_folder, 'validation.csv')\n    test_path = os.path.join(data_folder, 'test.csv')\n\ntrain_toxic_comment_bert_path = os.path.join(data_folder, 'jigsaw-toxic-comment-train-processed-seqlen128.csv') \nvalid_bert_path = os.path.join(data_folder, 'validation-processed-seqlen128.csv')\ntest_bert_path = os.path.join(data_folder, 'test-processed-seqlen128.csv')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def balance_data(data, ratio = 0.2):\n    '''\n    if we need to balance our data...\n    '''\n    inputs, labels = data\n    downsampled_inputs = []\n    downsampled_labels = []\n    for i in range(len(inputs)):\n        if labels[i] == 1.0 or np.random.rand() < ratio:\n            downsampled_inputs.append(inputs[i])\n            downsampled_labels.append(labels[i])\n\n    return downsampled_inputs, downsampled_labels","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_prepare(file_path, tokenizer, model_type, downsample_ratio = 0.2, max_len = 128, mode = 'train'):\n    '''\n    file_path: the path to input file. \n                In train mode, the input must be a tsv file that includes two columns where the first is text, and second column is label.\n                The first row must be header of columns.\n\n                In predict mode, the input must be a tsv file that includes only one column where the first is text.\n                The first row must be header of column.\n\n    tokenizer: BERT tokenizer\n    max_len: maximal length of input sequence\n    mode: train or predict\n    '''\n    # if we are in train mode, we will load two columns (i.e., text and label).\n    if mode == 'train':\n        # Use pandas to load dataset\n        df = pd.read_csv(file_path, header=0, \n                         names=['id', 'comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n        labels = df.toxic.values\n        # Create sentence and label lists\n        content = df.comment_text.values\n#         content, labels = balance_data((content, labels), downsample_ratio)\n        content, labels = content[:80000], labels[:80000]\n        \n        # Convert data into torch tensors\n        labels = torch.tensor(labels, dtype=torch.float)\n\n    elif mode == 'valid':\n        if translated:\n            df = pd.read_csv(file_path,header=0, \n                         names=['id', 'comment_text', 'lang', 'toxic', 'comment_text_en'])\n            content = df.comment_text_en.values\n        else:\n            df = pd.read_csv(file_path,header=0, \n                            names=['id', 'comment_text', 'lang', 'toxic'])\n            content = df.comment_text.values\n        \n        labels = df.toxic.values\n        # Convert data into torch tensors\n        labels = torch.tensor(labels, dtype=torch.float)\n        \n\n    # if we are in predict mode, we will load one column (i.e., text).\n    elif mode == 'predict':\n        if translated:\n            df = pd.read_csv(file_path,header=0, names=['id', 'content', 'lang', 'content_en'])\n            content = df.content_en.values\n        else:\n            df = pd.read_csv(file_path,header=0, names=['id', 'content', 'lang'])\n            content = df.content.values\n\n        # create placeholder\n        labels = []\n    else:\n        print(\"the type of mode should be either 'train', 'valid' or 'predict'. \")\n        return\n        \n\n    # We need to add a special token at the beginning for BERT/XLM-Roberta to work properly.\n    # Import the tokenizer, used to convert our text into tokens that correspond to BERT's/XLM-Roberta's vocabulary.\n    tokenized_texts = [tokenizer.tokenize(text) for text in content]\n    \n    if model_type == 'BERT':\n        cls_token = '[CLS]'\n        pad_token = '[PAD]'\n        eos_token = '[SEP]'\n    if model_type == 'XLMROBERTA':\n        cls_token = '<s>'\n        pad_token = '<pad>'\n        eos_token = '</s>'\n\n    # BERT: [CLS] + tokens + [SEP] + paddings\n    # XLM-Roberta: [CLS] + prefix_space + tokens + [SEP] + paddings\n    tokenized_texts = [[cls_token] + text for text in tokenized_texts]\n    \n    # if the sequence is longer the maximal length, we truncate it to the pre-defined maximal length\n    tokenized_texts = [ text[:max_len+1] for text in tokenized_texts]\n\n    # We also need to add a special token at the end.\n    tokenized_texts = [ text+[eos_token] for text in tokenized_texts]\n    \n    # Use the tokenizer to convert the tokens to their index numbers in the vocabulary\n    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n    # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n    pad_ind = tokenizer.convert_tokens_to_ids([pad_token])[0]\n    input_ids = pad_sequences(input_ids, maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n\n    # Create attention masks\n    attention_masks = []\n\n    # Create a mask of 1s for each token followed by 0s for pad tokens\n    for seq in input_ids:\n        seq_mask = [float(i != pad_ind) for i in seq]\n        attention_masks.append(seq_mask)\n\n    # Convert all of our data into torch tensors, the required datatype for our model\n    inputs = torch.tensor(input_ids)\n    masks = torch.tensor(attention_masks)\n\n    # Save it to csv file, if necessary\n    # inputs = [tuple(input_id) for input_id in input_ids]\n    # masks = [tuple(attention_mask) for attention_mask in attention_masks]\n    # df['input_word_ids'] = inputs\n    # df['input_mask'] = masks\n    # file_name = file_path.split('/')[-1]\n    # df.to_csv(os.path.join(data_folder, file_name[:-4]+'-processed-seqlen128-xmlroberta.csv'), index=False)\n\n    return inputs, labels, masks","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOAD DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(model_path, model_tokenizer, file_path, downsample_ratio = 0.2, batch_size = 32, max_len = 128, mode = 'train'):\n    '''\n    load data and split into batches\n    '''\n    # tokenizer from pre-trained model\n    tokenizer = model_tokenizer.from_pretrained(model_path)\n    \n    if model_path.startswith('xlm-roberta'):\n        inputs, labels, masks = data_prepare(file_path, tokenizer, downsample_ratio = downsample_ratio, model_type='XLMROBERTA', max_len = max_len, mode=mode)\n    elif model_path.startswith('bert'):\n        inputs, labels, masks = data_prepare(file_path, tokenizer, downsample_ratio = downsample_ratio, model_type='BERT', max_len=max_len, mode=mode)\n    \n    print(\"Data Size:\", len(inputs))\n    if mode == 'train':\n        data = TensorDataset(inputs, masks, labels)\n        dataloader = DataLoader(data, \n                                sampler = RandomSampler(data), # Select batches randomly\n                                batch_size=batch_size)\n    elif mode == 'valid':\n        data = TensorDataset(inputs, masks, labels)\n        dataloader = DataLoader(data, \n                                sampler = SequentialSampler(data), # Select batches randomly\n                                batch_size=batch_size)\n    elif mode == 'predict':\n        data = TensorDataset(inputs, masks)\n        dataloader = DataLoader(data, \n                                sampler = SequentialSampler(data), # Select batches randomly\n                                batch_size=batch_size)\n    else:\n        print(\"the type of mode should be either 'train', 'valid' or 'predict'. \")\n        return\n    return dataloader","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MODEL CLASS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# BCELOSS_VERSION\nclass Toxic_cls(nn.Module):\n    def __init__(self, model_path, model, hidden_size):\n        super(Toxic_cls, self).__init__()\n        self.model_path = model_path\n        self.hidden_size = hidden_size\n        self.model = model.from_pretrained(model_path, \n                                           output_hidden_states=False, \n#                                            output_attentions=True, \n                                           num_labels=1\n                                          )\n        self.label_num = 1\n        self.fc = nn.Linear(self.hidden_size, self.label_num)\n    \n    def forward(self, inputs, masks):\n        _, pooler_output = self.model(input_ids=inputs, attention_mask = masks)\n        fc_output = self.fc(pooler_output)\n\n        return fc_output\n\n# CROSSENTROPY_VERSION\n# class Toxic_cls(nn.Module):\n#     def __init__(self, model_path, model, hidden_size):\n#         super(Toxic_cls, self).__init__()\n#         self.model_path = model_path\n#         self.hidden_size = hidden_size\n#         self.model = model.from_pretrained(model_path, output_hidden_states=True, output_attentions=True)\n#         self.label_num = 2\n#         self.fc = nn.Linear(self.hidden_size, self.label_num)\n    \n#     def forward(self, inputs, masks):\n#         last_hidden_state, pooler_output, hidden_states, attentions = self.model(input_ids=inputs, attention_mask = masks)\n#         fc_output = self.fc(pooler_output)\n\n#         return fc_output, attentions","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HELPER FUNCTIONS (TRAIN, EVALUATION...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator, optimizer, scheduler, criterion):\n    \n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        \n        # Add batch to GPU\n        # batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        input_ids, input_mask, labels = batch\n        labels = labels.unsqueeze(1).type(torch.FloatTensor)\n        outputs = model(input_ids.to(device), input_mask.to(device))\n        \n        optimizer.zero_grad()\n        \n        loss = criterion(outputs, labels.to(device))\n        # delete used variables to free GPU memory\n        del batch, input_ids, input_mask, labels\n        loss.backward()\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        if device_type == 'tpu':\n            xm.optimizer_step(optimizer, barrier=True)\n        else:\n            optimizer.step()\n        scheduler.step()\n        epoch_loss += loss.cpu().item()\n    \n    # free GPU memory\n    if device == 'cuda':\n        torch.cuda.empty_cache()\n\n    return epoch_loss / len(iterator)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    all_pred=[]\n    all_label = []\n    \n    with torch.no_grad():\n    \n        for i, batch in enumerate(iterator):\n\n            # Add batch to GPU\n            # batch = tuple(t.to(device) for t in batch)\n            # Unpack the inputs from our dataloader\n\n            input_ids, input_mask, labels = batch\n            labels = labels.unsqueeze(1).type(torch.FloatTensor)\n            outputs = model(input_ids.to(device), input_mask.to(device))\n            \n            loss = criterion(outputs, labels.to(device))\n\n            # delete used variables to free GPU memory\n            del batch, input_ids, input_mask\n            epoch_loss += loss.cpu().item()\n\n            # identify the predicted class for each example in the batch\n            # probabilities, predicted = torch.max(outputs.cpu().data, 1)\n            # put all the true labels and predictions to two lists\n            gold = labels.cpu().detach().numpy().tolist()\n            predicted = outputs.cpu().detach().numpy().tolist()\n            all_pred.extend(predicted)\n            all_label.extend(gold)\n    \n#     accuracy = accuracy_score(all_label, all_pred)\n#     f1score = f1_score(all_label, all_pred, average='macro') \n#     roc_auc = roc_auc_score(all_label, all_pred, average = 'macro')\n    roc_auc = roc_auc_score(np.array(all_label) >= 0.5, all_pred, average = 'macro')\n    return epoch_loss / len(iterator), 0, 0, roc_auc","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MAIN FUNCTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\nlr = 1e-5\nmax_grad_norm = 1.0\nepochs = 2\nwarmup_proportion = 0.1\ndownsample_ratio = 0.1\nhidden_size = 768","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Xlm-Roberta\nused_model, used_tokenizer, model_path = MODELS[2]\n\n# Xlm-Roberta-large\n# used_model, used_tokenizer, model_path = MODELS[3]\n\n# Bert\n# used_model, used_tokenizer, model_path = MODELS[0]\n\ntrain_dataloader = load_data(model_path, used_tokenizer, train_toxic_comment_path, downsample_ratio = downsample_ratio,  batch_size = batch_size, mode='train')\nvalid_dataloader = load_data(model_path, used_tokenizer, valid_path, downsample_ratio = downsample_ratio, batch_size=batch_size, mode = 'valid')\n","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a8fcb280cf40c4a9ec61eb795e88c9"}},"metadata":{}},{"output_type":"stream","text":"\nData Size: 80000\nData Size: 8000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_training_steps  = len(train_dataloader) * epochs\nnum_warmup_steps = num_training_steps * warmup_proportion\n\n\nmodel = Toxic_cls(model_path, used_model, hidden_size).to(device)\n\n### In Transformers, optimizer and schedules are instantiated like this:\n# Note: AdamW is a class from the huggingface library\n# the 'W' stands for 'Weight Decay\"\noptimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n# schedules\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n\n# We use nn.CrossEntropyLoss() as our loss function. \n# criterion = nn.CrossEntropyLoss()\ncriterion = nn.BCEWithLogitsLoss()","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=737.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0899605d65ac44afa4a6918659874216"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c9fb2193934449b8e0e0462ee4a2d7"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":20,"outputs":[{"output_type":"stream","text":"The model has 278,044,417 trainable parameters\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\n# train model\nloss_list = []\nacc_list = []\nbest_auc = 0\nfor epoch in trange(epochs, desc=\"Epoch\"):\n    start_time = time.time()\n    train_loss = train(model, train_dataloader, optimizer, scheduler, criterion)  \n    val_loss, _, _, val_auc = evaluate(model, valid_dataloader, criterion)\n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    # Create checkpoint at end of each epoch\n    state = {\n        'epoch': epoch,\n        'state_dict': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict()\n        }\n    if best_auc < val_auc:\n        best_auc = val_auc\n        torch.save(state, \"./BEST_MODEL.pt\")\n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print('\\tTrain Loss: {:.4f}, Validation AUC: {:.4f}'.format(train_loss, val_auc))","execution_count":24,"outputs":[{"output_type":"stream","text":"Epoch:  50%|█████     | 1/2 [18:16<18:16, 1096.90s/it]","name":"stderr"},{"output_type":"stream","text":"Epoch: 01 | Time: 18m 8s\n\tTrain Loss: 0.1293, Validation AUC: 0.9211\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 100%|██████████| 2/2 [36:41<00:00, 1100.51s/it]","name":"stderr"},{"output_type":"stream","text":"Epoch: 02 | Time: 18m 8s\n\tTrain Loss: 0.0746, Validation AUC: 0.9235\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### PREDICT TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n# used_model, used_tokenizer, model_path = MODELS[0]\n\n# lr = 2e-5\n# max_grad_norm = 1.0\n# hidden_size = 768\n\nmodel_best = Toxic_cls(model_path, used_model, hidden_size).to(device)\nmodel_best.load_state_dict(torch.load('./BEST_MODEL.pt')['state_dict'])\n\ntest_dataloader = load_data(model_path, used_tokenizer, test_path, batch_size=batch_size, mode='predict')\n\nall_pred=[]\n    \nwith torch.no_grad():\n\n    for i, batch in enumerate(test_dataloader):\n\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        input_ids, input_mask= batch\n        outputs = model_best(input_ids, input_mask)\n        # delete used variables to free GPU memory\n        del batch, input_ids, input_mask\n        # identify the predicted class for each example in the batch\n#         probabilities, predicted = torch.max(outputs.cpu().data, 1)\n        predicted = outputs.cpu().detach().numpy().tolist()\n        # put all the true labels and predictions to two lists\n        all_pred.extend(predicted)","execution_count":26,"outputs":[{"output_type":"stream","text":"Data Size: 63812\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(os.path.join(data_folder, 'sample_submission.csv'))\nsub['toxic'] = [1.0 if float(t[0]) >= 0 else 0.0 for t in all_pred]\nsub.to_csv('./submission.csv', index=False)\nsub.head()","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"   id  toxic\n0   0    0.0\n1   1    0.0\n2   2    0.0\n3   3    0.0\n4   4    0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}