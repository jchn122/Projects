
[Model](https://github.com/jchn122/Projects/blob/master/Jigsaw_Toxic_Comments/jigsaw-multilingual-toxic-comment-classification.ipynb)

[Report](https://github.com/jchn122/Projects/blob/master/Jigsaw_Toxic_Comments/Jigsaw%20Multilingual%20Toxic%20Comment%20Classification.pdf)

## Introduction

[Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/overview/description) is Kaggle competition launched by [Jigsaw](https://jigsaw.google.com/) and Google. This is a text classification task which aims to identify whether or not a comment is toxic. Toxicity here means rude, disrespectful or any inappropriate comments that will make people uncomfortable. In this project, our inputs are multilingual comments and we are expected to predict if it is toxic.

## Motivation and Contributions/Originality
Toxic comments have always been an issue online, because it only takes one to sour things and derail discussions. Given the prominence of social media nowadays, the growing impacts of toxic comments are becoming more concerning. For example, South Korean singer/actress Sulli's suicide was supposedly linked to depression caused by cyberbullying. Throughout her career, she had to constantly battle toxic, malicious comments directed towards her online. Unfortunately, Sulli's situation only came to light because of her status and it's only the tip of the iceberg. We hope to contribute by developing a system that is more robust than existing toxic-comment detecting models.

## Data

All the data is available on Kaggle. Click [here](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data) to access the datasets.
